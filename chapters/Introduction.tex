  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% -*- coding: utf-8; mode: latex -*- %%
  %
%%%%%                         CHAPTER
 %%%
  %

% $Id: 1020-lorem-ipsum.tex,v 1.2 2009/06/19 15:51:46 david Exp $
% $Log: 1020-lorem-ipsum.tex,v $
% Revision 1.2  2009/06/19 15:51:46  david
% *** empty log message ***
%
% Revision 1.1  2007/11/23 09:52:39  david
% *** empty log message ***
%
%

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
%%%%%                           HEAD MATTER
 %%%
  %

\chapter{Introduction}
%\addcontentsline{lof}{chapter}{\thechapter\quad Lorem Ipsum}
%\addcontentsline{lot}{chapter}{\thechapter\quad Lorem Ipsum}
\label{ch:Introduction}

%\begin{quotation}
%  {\small\it Neque porro quisquam est qui dolorem ipsum quia dolor sit amet, consectetur, adipisci velit...}
%
%{\small\it -- Cerico}
%\end{quotation}



  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
%%%%%                        FIRST SECTION
 %%%
  %

\section{Motivation}

\subsection{The De Facto Industrial Standard in Big Data Era}

The \textit{Apache Hadoop}~\cite{apachehadoop} ecosystem has become the de facto industrial standard to store, process and analyze large data sets in the big data era~\cite{cloudera}. It is widely used as a computational platform for a variety of areas including search engines, data warehousing, behavioral analysis, natural language processing, genomic analysis, image processing, etc~\cite{shvachko2011apache}. 

\noindent The \textit{Hadoop Distributed File System} (HDFS) is the storage layer for Apache Hadoop, which enables petabytes of data to be persisted on clusters of commodity hardware at relatively low cost~\cite{borthakur2008hdfs}. Inspired by the \textit{Google File System} (GFS)~\cite{ghemawat2003google}, the namespace, \textit{metadata}, is decoupled from data and stored in-memory on a single server, called the \textit{NameNode}. The file datasets are stored as sequences of blocks and replicated across potentially thousands of machines for fault tolerance.

\subsection{Limits to growth in HDFS}

Built upon the single namespace server, \textit{the NameNode}, architecture, one well-known limitation of HDFS is the limitation to growth~\cite{shvachko2010hdfs}. Since the metadata is kept in-memory for fast operation in NameNode, the number of file objects in the filesystem is limited by the amount of memory of the NameNode. 

\noindent Approximately, the size of the metadata for a single file object having two blocks (replicated three times by default) is 600 bytes. As a rule of thumb, for one petabyte physical storage, it requires one gigabyte metadata in memory~\cite{shvachko2010hdfs}. Table~\ref{table:memoryRequirement} gives an estimation of the memory requirement and its related physical storage capacity for different number of files.

\begin{table}[h]
	\centering
	\begin{tabular}{|c|c|c|}
		\hline
		\textbf{Number of Files} & \textbf{Memory Requirement} & \textbf{Physical Storage} \\ \hline
		1 million       & 0.6 GB             & 0.6 PB           \\ \hline
		100 million     & 60 GB              & 60 PB            \\ \hline
		1 billion       & 600 GB             & 600 PB           \\ \hline
		2 billion       & 1200 GB            & 1200 PB          \\ \hline
	\end{tabular}
	\caption{Memory Requirement for Related Storage Capacity in HDFS}
	\label{table:memoryRequirement}
\end{table}

\noindent As HDFS runs in the \textit{Java Virtual Machine} (JVM), due to interactive workloads, heap sizes larger than 60 GB is not considered practical~\cite{shvachko2010hdfs}. Therefore, 100 million files will be the maximum storage capacity of HDFS.

\subsection{Hop-HDFS and Its Limitation}

The \textit{Hadoop Open Platform-as-a-service} (Hop) ~\cite{hop} is an open platform-as-a-Service (PaaS) support of the Hadoop ecosystem on existing cloud platforms including Amazon Web Service and OpenStack. The storage layer of Hop, called the Hop-HDFS, is a highly available implementation of HDFS, based on storing the metadata in a distributed, in-memory, replicated database, called the \textit{MySQL Cluster}. It aims to overcome the NameNode's limitation while maintaining the strong consistency semantics of HDFS so that applications written for HDFS can run on Hop-HDFS without 
modifications. 

\noindent Precedent thesis works have contributed for a transaction model~\cite{wasif2012distributed} ~\cite{peiro2013maintaining} as well as a high availability multi-NameNode architecture~\cite{d2013kthfs} for Hop-HDFS. It can store up to 4.1 billion files with 3TB MySQL Cluster support for metadata~\cite{hakimzadeh2014scaling}. 

\noindent However, in HDFS, the correctness and consistency of the namespace is ensured by atomic metadata mutation~\cite{shvachko2010hadoop}. In order to maintain the same level of strong consistency semantics, system-level coarse grained locking and row-level fine grained locking are adopted in precedent projects of Hop-HDFS, but the overall performance is heavily restricted compared to the original HDFS. Therefore, investigation for better concurrency control to improve the performance of Hop-HDFS is the main motivation.
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
%%%%%                      SECOND SECTION
 %%%
  %

% TODO?
\section{Problem Statement}

In HDFS, the NameNode's operations are categorized into \textit{read} or \textit{write} operations. To protect the metadata among parallel running threads, a global read/write lock is used to maintain the atomicity of the namespace. We call it \textit{system-level lock}. Concurrent threads to access shared object for read operations are allowed, but it restricts a single thread to access object for write operations. This \textit{single-writer-multiple-readers} concurrency model will not reduce the throughput much since the metadata is kept optimized data structures in-memory~\cite{hakimzadeh2014scaling}.

\noindent The first version of Hop-HDFS, called the KTHFS~\cite{wasif2012distributed}, adopts the system-level locking mechanism to serialize transactions. All concurrent readers get the same view of the mutated data reflected by completed writes. We call it \textit{Strong Consistency}.


  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
%%%%%                         ANOTHER SECTION
 %%%
  %
\section{Contribution}

CCC

  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
  %
%%%%%                          LAST SECTION
 %%%
  %

\section{Document Structure}

[To be Added]

  %
 %%%
%%%%%                        THE END
  %
  %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: "tese"
%%% End: 
